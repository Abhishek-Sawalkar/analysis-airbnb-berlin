{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\n# import plotly.graph_objects as go\nimport plotly.express as px\nimport plotly\nfrom plotly.offline import plot, iplot, init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, f1_score, auc\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport folium\nfrom folium.plugins import HeatMapWithTime\nfrom folium.plugins import HeatMap\nfrom datetime import datetime\nfrom branca.colormap import linear\nfrom matplotlib import cm\n\nimport geopy.distance\nimport requests\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-23T13:15:19.748558Z","iopub.execute_input":"2022-02-23T13:15:19.749302Z","iopub.status.idle":"2022-02-23T13:15:19.758075Z","shell.execute_reply.started":"2022-02-23T13:15:19.749265Z","shell.execute_reply":"2022-02-23T13:15:19.757505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/berlin-airbnb/'\n\n# geojson from https://github.com/ljwolf/geopython/blob/master/data/berlin-neighbourhoods.geojson\ngeo_df = gpd.read_file(data_path + 'neighbourhoods.geojson')\nlistings_df = pd.read_csv(data_path + 'listings.csv', low_memory=False)\nreviews_df = pd.read_csv(data_path + 'reviews.csv',parse_dates = [2])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:15:20.094572Z","iopub.execute_input":"2022-02-23T13:15:20.095134Z","iopub.status.idle":"2022-02-23T13:15:24.297149Z","shell.execute_reply.started":"2022-02-23T13:15:20.095085Z","shell.execute_reply":"2022-02-23T13:15:24.296252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 1:\nWhat proportion of Airbnb hosts in Berlin likely use hosting as a primary source of income (or are businesses)? \n\n## Data Preparation:","metadata":{}},{"cell_type":"code","source":"# listings_df.groupby('calculated_host_listings_count')['room_type'].agg(set).values","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:15:24.298748Z","iopub.execute_input":"2022-02-23T13:15:24.299408Z","iopub.status.idle":"2022-02-23T13:15:24.305302Z","shell.execute_reply.started":"2022-02-23T13:15:24.29937Z","shell.execute_reply":"2022-02-23T13:15:24.304692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"availability_room_type = pd.DataFrame(listings_df.groupby('room_type')['availability_365'].mean()).reset_index()\navailability_room_type['availability_365'] = availability_room_type['availability_365'].apply(lambda x: round(x))\nfig = px.bar(availability_room_type, x='room_type', y='availability_365',\n      hover_data = ['availability_365'], labels = {'labels': 'Type of Listings',\n                                                    'availability_365': 'Average Availability per Year'})\nfig.update_layout(\n    yaxis_title=\"Average Number of Days Available in Year\",\n    xaxis_title = \"Type of Listings\",\n    title = \"Average Availability of Listings Based on Type of Listing\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:15:32.541425Z","iopub.execute_input":"2022-02-23T13:15:32.541874Z","iopub.status.idle":"2022-02-23T13:15:33.617396Z","shell.execute_reply.started":"2022-02-23T13:15:32.541829Z","shell.execute_reply":"2022-02-23T13:15:33.616395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([('Entire home/apt',85),\n        ('Hotel room',275),\n        ('Private room',55),\n        ('Shared room',114)])\ndf =df.rename(columns={0:'room_type',1:'availability_365'})","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:44:51.733047Z","iopub.execute_input":"2022-02-23T13:44:51.733981Z","iopub.status.idle":"2022-02-23T13:44:51.74652Z","shell.execute_reply.started":"2022-02-23T13:44:51.733934Z","shell.execute_reply":"2022-02-23T13:44:51.745705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"availability_room_type","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:15:48.977986Z","iopub.execute_input":"2022-02-23T13:15:48.978288Z","iopub.status.idle":"2022-02-23T13:15:48.992569Z","shell.execute_reply.started":"2022-02-23T13:15:48.978254Z","shell.execute_reply":"2022-02-23T13:15:48.991787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean the price column (change to numeric)\nlistings_df['price'] = pd.to_numeric(listings_df.price.apply(\n    lambda x: x.replace(\"$\", \"\").replace(\",\",\"\")))\nroom_types = pd.merge(listings_df.room_type.value_counts(), listings_df.groupby('room_type').agg({'price': 'mean'})['price'], left_index=True, right_index=True).reset_index()\nroom_types['Price'] = room_types['price'].apply(lambda x: round(x))\n\nfig = go.Figure(\n    data=[\n        go.Bar(name='No. of Listings', x=room_types['index'], y=room_types['room_type'], yaxis='y', offsetgroup=1),\n        go.Bar(name='Price Paid', x=room_types['index'], y=room_types['Price'], yaxis='y2', offsetgroup=2)\n    ],\n    layout={\n        'yaxis': {'title': 'Number of Listings'},\n#         'xaxis' : {\"Room Type\"},\n        'yaxis2': {'title': 'Price Paid( $ )', 'overlaying': 'y', 'side': 'right'}\n    }\n)\n\n# Change the bar mode\nfig.update_layout(\n    xaxis_title = \"Room Type\"\n    )\nfig.update_layout(barmode='group')\nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Number_of_Listings_Relative_to_Avg_Price_each_Room_Type.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:15:51.250428Z","iopub.execute_input":"2022-02-23T13:15:51.251435Z","iopub.status.idle":"2022-02-23T13:15:51.363974Z","shell.execute_reply.started":"2022-02-23T13:15:51.251385Z","shell.execute_reply":"2022-02-23T13:15:51.363174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"room_types","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:26:37.319429Z","iopub.execute_input":"2022-02-23T13:26:37.320248Z","iopub.status.idle":"2022-02-23T13:26:37.330006Z","shell.execute_reply.started":"2022-02-23T13:26:37.320192Z","shell.execute_reply":"2022-02-23T13:26:37.32947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A host can have multiple entries in listing_df if his/her listings is also present in different neighbourhood_cleansed","metadata":{}},{"cell_type":"code","source":"listings_df[listings_df.duplicated('host_id')][['host_id', 'calculated_host_listings_count', 'neighbourhood_cleansed']].sort_values('host_id')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:17:12.798254Z","iopub.execute_input":"2022-02-23T13:17:12.798552Z","iopub.status.idle":"2022-02-23T13:17:12.830136Z","shell.execute_reply.started":"2022-02-23T13:17:12.798519Z","shell.execute_reply":"2022-02-23T13:17:12.829512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_listings_info_df(bins, labels, hosts = False):\n    '''\n    Function to make a dataframe with average values of availability and \n    listing numbers based on the bins chosen\n    Inputs: bins - a pd.IntervalIndex (pandas IntervalIndex) consisting of the cuts you \n                    want to divide the listing\n                    counts into\n            labels - a List of strings with each corresponding to each provided \n                        interval in 'bins'\n            hosts - If False (default), just get the percentage of listings that\n                    correspond to the bin, e.g. if bin is 2 listings, then \n                    get information for all listings that have a host with 2 listings.\n                    If True, get information for hosts, e.g. if bin is 2 listings,\n                    get information for all hosts that have 2 listings.\n    Output: a Pandas dataframe containing the labels and bins with the corresponding \n            percentage of listings (or hosts if hosts=True) in each bin along\n            with the average availability (/365 days)\n            of all the listings in that bin\n    '''\n    if hosts:\n#         df = listings_df.groupby('host_id').agg({'calculated_host_listings_count':'sum', 'availability_365':'mean'})\n        df = listings_df.groupby('host_id')[['calculated_host_listings_count',\n                                             'availability_365']].mean()\n#         print(df['calculated_host_listings_count'].value_counts())\n        \n    else:\n        df = listings_df.copy()\n        \n#    print(df.shape)\n        \n    # make a new column with the group/cut that the listing belongs to (based on the number of \n    # listings the host has)\n    \n    # calculated_host_listings_count: The number of listings(total number of rooms a host has) the host has in the current scrape, in the city/region geography.\n    df['num_listings_cut'] = pd.cut(df.calculated_host_listings_count, bins)\n\n    # get percentage of listings that are the only listing vs. one of many listings\n    num_listings_percent = df.num_listings_cut.value_counts(normalize=True)\n\n    # get the average availability out of the calendar year that each category offers\n    availability_num_listings = pd.DataFrame(df.groupby('num_listings_cut')['availability_365'].mean()).reset_index()\n\n    # make dataframe that contains the listing percentages by type of host (host listing num) \n    # using 'num_listings_perc'\n    df = pd.DataFrame(num_listings_percent).reset_index().rename(columns = {'index': 'cut', \n                                                                            'num_listings_cut': 'perc_listings'})\n    # make sure the categories/cuts are in order\n    df = df.sort_values(by='cut').reset_index(drop=True) \n\n    # merge the string version of groups/cuts into dataframe:\n    df = pd.concat([labels, df], axis=1).rename(columns = {0: 'labels'})\n\n    # include the average availability for each of these cuts into the dataframe \n    df = df.merge(availability_num_listings, left_on='cut', \n                           right_on='num_listings_cut').drop(columns = 'num_listings_cut')\n    \n    # make a new column for the listing percentages cleaned (rounded to 4 digits and in percentage format)\n    df['perc_listings_clean'] = df.perc_listings.apply(\n        lambda x: 100*round(x,4))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:17:13.258365Z","iopub.execute_input":"2022-02-23T13:17:13.258618Z","iopub.status.idle":"2022-02-23T13:17:13.269601Z","shell.execute_reply.started":"2022-02-23T13:17:13.25859Z","shell.execute_reply":"2022-02-23T13:17:13.268644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results/ Visualisations:","metadata":{}},{"cell_type":"code","source":"\n# split the total number of listings that the host has into different groups\nbins = pd.IntervalIndex.from_tuples([(-1, 1), (1, 3),(3,10),(10,100)])\n\n# make a list with strings representing each of the cuts -- the interval is (closed, open] \n# meaning (2,3] contains only 3\nlabels = pd.Series(['1 Listing','2-3 Listings','4-10 Listings','11+ Listings'])\n\n# create the dataframe with information about listings based on \n# how many listings the host has\nlistings_info_df = make_listings_info_df(bins, labels)\n# rename some of the columns to make it more readable in the plot\nlistings_info_df = listings_info_df.rename(columns = {'labels': 'Host Listings',\n                                                     'perc_listings': 'Percentage of Listings'})\nlistings_info_df","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:17:15.741844Z","iopub.execute_input":"2022-02-23T13:17:15.742167Z","iopub.status.idle":"2022-02-23T13:17:15.834272Z","shell.execute_reply.started":"2022-02-23T13:17:15.742133Z","shell.execute_reply":"2022-02-23T13:17:15.83335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a pie chart of the breakdown of listings\nfig = px.pie(listings_info_df, names = 'Host Listings', values='Percentage of Listings', \n             title='Breakdown of Listings by Host\\'s Total Number of Listings(Area Based)',\n            color_discrete_sequence = px.colors.sequential.Emrld,\n             hover_data=['Host Listings'], labels={'Host Listings':'Host Listings'})\nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Breakdown_of_Listings.html')\n\n# Now group the dataframe of listings based on the number of listings each host has, and look at the mean \n# number of listings in each category\nhosts_df = listings_df.groupby('host_id')[['calculated_host_listings_count',\n                                           'availability_365']].mean()\nhosts_df['num_listings_cut'] = pd.cut(hosts_df.calculated_host_listings_count, bins)\nhosts_df.groupby('num_listings_cut')['calculated_host_listings_count'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:34:55.979657Z","iopub.execute_input":"2022-02-23T13:34:55.97994Z","iopub.status.idle":"2022-02-23T13:34:56.010483Z","shell.execute_reply.started":"2022-02-23T13:34:55.979904Z","shell.execute_reply":"2022-02-23T13:34:56.009705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([('1 Listing','(-1, 1]',0.736727,48.878850,73.67),\n                    ('2-3 Listings','(1, 3]',0.162454,91.673924,16.25),\n                    ('4-10 Listings','(3, 10]',0.063861,206.164281,6.39),\n                    ('11+ Listings','(10, 100]',0.036958,236.570968,3.70)])\n# df.columns = {\"Host Listings\",'cut','Percentage of Listings','availability_365','perc_listings_clean'}\n# # rename some of the columns to make it more readable in the plot\ndf = df.rename(columns={0:\"Host Listings\",1:'cut',2:'Percentage of Listings',3:'availability_365',4:'perc_listings_clean'})\ndf","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:39:58.50701Z","iopub.execute_input":"2022-02-23T13:39:58.507828Z","iopub.status.idle":"2022-02-23T13:39:58.523763Z","shell.execute_reply.started":"2022-02-23T13:39:58.507789Z","shell.execute_reply":"2022-02-23T13:39:58.522678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:36:09.508051Z","iopub.execute_input":"2022-02-23T13:36:09.508361Z","iopub.status.idle":"2022-02-23T13:36:09.526833Z","shell.execute_reply.started":"2022-02-23T13:36:09.508328Z","shell.execute_reply":"2022-02-23T13:36:09.525841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Still grouping the dataframe on the basis of hosts (not listings) we create \n# a dataframe containing \n# information about the hosts based on how many listings they have\n\n# define the intervals we want to split on\nbins = pd.IntervalIndex.from_tuples([(0, 1), (1, 3), (3,100)])\n# associate bins with labels\nlabels = pd.Series(['1 Listing','2-3 Listings','4+ Listings'])\n\n# make the host info dataframe\nhosts_info_df = make_listings_info_df(bins, labels, hosts=True)\nprint(hosts_info_df)\n# plot the host info dataframe based on how many listings the different categories of hosts have\n\nfig = px.pie(hosts_info_df, names = 'labels', values='perc_listings_clean', \n             title='Breakdown of Hosts by Number of Listings',\n            color_discrete_sequence = px.colors.sequential.Emrld,\n             #hover_name = 'labels',\n             hover_data = ['perc_listings_clean'],\n            labels ={'labels': 'Num Listings','perc_listings_clean': 'Percentage'})\nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Breakdown_of_Listings.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:17:32.344767Z","iopub.execute_input":"2022-02-23T13:17:32.345053Z","iopub.status.idle":"2022-02-23T13:17:32.463839Z","shell.execute_reply.started":"2022-02-23T13:17:32.34502Z","shell.execute_reply":"2022-02-23T13:17:32.462958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make the same hosts info dataframe but for different intervals to look at \n# average availability of listings\n# throughout the year\n\nbins = pd.IntervalIndex.from_tuples([(-1, 1), (1, 2), (2,3),(3,4),(4, 100)])\n\nlabels = pd.Series(['1 Listing','2 Listings','3 Listings','4 Listings','5+ Listings'])\n\n# make the host info dataframe\nhosts_info_df = make_listings_info_df(bins, labels, hosts=True)\n# change formatted percentage of listings for input into the bar plot\nhosts_info_df['perc_listings_clean'] = hosts_info_df.perc_listings.apply(\n    lambda x: \"Represents {:.2f}% of Listings\".format(x*100))\nhosts_info_df['availability_365'] = hosts_info_df['availability_365'].apply(lambda x: round(x))\nprint(hosts_info_df)\n# plot results using Plotly express\n\nfig = px.bar(hosts_info_df, x='labels', y='availability_365',\n      hover_data = ['perc_listings_clean'], labels = {'labels': 'Num Listings',\n                                                    'availability_365': 'Average Availability per Year',\n                                                   'perc_listings_clean': 'Percentage'})\nfig.update_layout(\n    yaxis_title=\"Average Number of Days Available in Year\",\n    xaxis_title = \"Host's Number of Listings\",\n    title = \"Host's Average Availability by Number of Listings\")\n\nfig.show()\n# Saving the plot\nplotly.offline.plot(fig, filename='Hosts_Average_Availability.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:17:36.226875Z","iopub.execute_input":"2022-02-23T13:17:36.227309Z","iopub.status.idle":"2022-02-23T13:17:36.535397Z","shell.execute_reply.started":"2022-02-23T13:17:36.227267Z","shell.execute_reply":"2022-02-23T13:17:36.534503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([('1 Listing','(-1, 1]',0.893015,49,'Represents 89.30% of Listings'),\n        ('2 Listings','(1, 2]',0.075193,85,'Represents 7.52% of Listings'),\n        ('3 Listings''(2, 3]',0.015511,114,'Represents 1.55% of Listings'),\n        ('4 Listings','(3, 4]',0.005684,179,'Represents 0.57% of Listings'),\n        ('5+ Listings','(4, 100]',0.010597,225,'Represents 1.06% of Listings')])\ndf = df.rename(columns={0: 'labels',1:'cut',2:'perc_listings',3:'availability_365',4:'perc_listings_clean'})\n\ndf","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:51:48.048444Z","iopub.execute_input":"2022-02-23T13:51:48.048935Z","iopub.status.idle":"2022-02-23T13:51:48.063941Z","shell.execute_reply.started":"2022-02-23T13:51:48.0489Z","shell.execute_reply":"2022-02-23T13:51:48.062963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hosts_info_df","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:17:56.072349Z","iopub.execute_input":"2022-02-23T13:17:56.072643Z","iopub.status.idle":"2022-02-23T13:17:56.085065Z","shell.execute_reply.started":"2022-02-23T13:17:56.072613Z","shell.execute_reply":"2022-02-23T13:17:56.08431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Avaliability_x. The availability of the listing x days in the future as determined by the calendar. **NOTE** a listing may not be available because it has been booked by a guest or blocked by the host.","metadata":{}},{"cell_type":"markdown","source":"# Question 2:\nHow does the price of an Airbnb differ throughout neighbourhoods in Berlin, and what neighbourhoods have the best price for value?\n\n## Data Understanding:","metadata":{}},{"cell_type":"code","source":"listings_df.shape, reviews_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:00.281304Z","iopub.execute_input":"2022-02-22T19:33:00.282506Z","iopub.status.idle":"2022-02-22T19:33:00.289971Z","shell.execute_reply.started":"2022-02-22T19:33:00.282456Z","shell.execute_reply":"2022-02-22T19:33:00.288669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean the price column (change to numeric)\n# listings_df['price'] = pd.to_numeric(listings_df.price.apply(\n#     lambda x: x.replace(\"$\", \"\").replace(\",\",\"\")))\n# look at breakdown of prices in different districts\nlistings_df.groupby('neighbourhood_group_cleansed').agg({'price':['mean',\n                                                                  'median','std','count']})","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:00.291619Z","iopub.execute_input":"2022-02-22T19:33:00.292338Z","iopub.status.idle":"2022-02-22T19:33:00.320159Z","shell.execute_reply.started":"2022-02-22T19:33:00.292289Z","shell.execute_reply":"2022-02-22T19:33:00.319549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation:","metadata":{}},{"cell_type":"code","source":"# listings_df[listings_df.name.str.contains('zentral gelegen|WG|Axel|Friedrichshain|Hackescher') == True].price.unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:00.321559Z","iopub.execute_input":"2022-02-22T19:33:00.321835Z","iopub.status.idle":"2022-02-22T19:33:00.332042Z","shell.execute_reply.started":"2022-02-22T19:33:00.321807Z","shell.execute_reply":"2022-02-22T19:33:00.331111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(listings_df, y=\"price\", title=\"Overall Price Distribution With Outliers\")\n# fig.title(\"Price distribution\") \nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Overall_Price_Distribution_With_Outliers.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:00.333376Z","iopub.execute_input":"2022-02-22T19:33:00.333786Z","iopub.status.idle":"2022-02-22T19:33:00.588943Z","shell.execute_reply.started":"2022-02-22T19:33:00.333742Z","shell.execute_reply":"2022-02-22T19:33:00.587814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the year and month that each review was made in new columns, so we can split by date easier\n\nreviews_df['year'] = reviews_df.date.dt.year\nreviews_df['month'] = reviews_df.date.dt.month\nreviews_df = reviews_df.sort_values(by='date', ascending=True).reset_index(drop=True)\n# CLEAN the listings_df price data, done mostly by investigating manually the \n# extremely expensive outliers \n# some hotels and hostels list all listings as 1000 or 999 a night, so we\n# removed these, as well with some individual shared rooms listing their monthly prices\n\n\n# set these falsly labelled property prices to nan\nlistings_df.loc[(listings_df.price > 2000) & \n                (listings_df.name.str.contains('zentral gelegen|WG|Axel|Friedrichshain|Hackescher|Zentrum')),\n                'price'] = np.nan\n\n# set the dorm rooms prices to nan, as they do not cost 1000 a night\nlistings_df.loc[(listings_df.price == 1000) & (listings_df.name.str.contains('Dorm')),'price'] = np.nan\n\n# target individual properties that was not accurately priced\nlistings_df.loc[(listings_df.price == 1000) & \n                (listings_df.name.str.contains('Generator|Private|MAMA|Quiet')), 'price'] = np.nan\n\nlistings_df.loc[(listings_df.price == 1000) & \n                (listings_df.summary.str.contains('Base yourself')), 'price'] = np.nan\n\n# found all those priced at 999 dollars were not accurate\nlistings_df.loc[(listings_df.price == 999),'price'] = np.nan\n# can see that all the remaining properties above 900$ a night are accurately\n# priced, so we can take averages without including false outliers\nlistings_df[listings_df.price > 900].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:00.590497Z","iopub.execute_input":"2022-02-22T19:33:00.590812Z","iopub.status.idle":"2022-02-22T19:33:01.275834Z","shell.execute_reply.started":"2022-02-22T19:33:00.590774Z","shell.execute_reply":"2022-02-22T19:33:01.274843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(listings_df, y=\"price\", title=\"Overall Price Distribution After Removing Outliers\")\n# fig.title(\"Price distribution\") \nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Overall_Price_Distribution_After_Removing_Outliers.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:01.277891Z","iopub.execute_input":"2022-02-22T19:33:01.278543Z","iopub.status.idle":"2022-02-22T19:33:01.511287Z","shell.execute_reply.started":"2022-02-22T19:33:01.278497Z","shell.execute_reply":"2022-02-22T19:33:01.510005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataframe combining the reviews and certain information from the listings dataframe,\n# so now we have a dataframe only with listings that have been reviewed at least once\n\nreviews_neighbourhood = reviews_df[['listing_id',\n                                    'comments',\n                                    'date','year',\n                                    'month']].copy().merge(listings_df[['id',\n                                                                        'neighbourhood_group_cleansed',\n                                                                        'neighbourhood_cleansed',\n                                                                        'price','name',\n                                                                        'space','summary',\n                                                                        'latitude','longitude']].copy(),\n                                                                  left_on='listing_id',\n                                                                  right_on='id',\n                                                                  how='inner').drop(columns = 'id')\n# take only those reviews from 2019 and later so we have accurate prices paid\n# make a dataframe with all reviews and (approximate) prices paid for those stays and group by \n# neighbourhood\n\n# FOR THE neighbourhood_cleansed\nreviews_grouped = reviews_neighbourhood[reviews_neighbourhood.year >= \n                                        2019].groupby(['neighbourhood_cleansed'])[['price']].agg(['mean','median',\n                                                                                                  'count'])\n# round the price to 2 decimal points\nreviews_grouped['price']  = reviews_grouped.price.apply(lambda x: round(x,2))\n# clean up the column names\nreviews_grouped.columns = ['_'.join(col).strip() for col in reviews_grouped.columns.values]\n\n# add neighbourhood_group_cleansed\n\nreviews_grouped = reviews_grouped.merge(reviews_neighbourhood[['neighbourhood_cleansed', 'neighbourhood_group_cleansed']].drop_duplicates(), \n                                        left_on = 'neighbourhood_cleansed', right_on='neighbourhood_cleansed')\n\n# if a neighbourhood has too little reviews to take the mean price paid, set to NAN so it's not included in map\nreviews_grouped.loc[reviews_grouped.price_count < 40,'price_mean'] = np.nan\n\nreviews_grouped.reset_index(inplace=True)\n\nprint(reviews_grouped.head(), reviews_grouped.shape)\n\n## FOR THE neighbourhood_grouped_cleansed\n\n# reviews_grouped_1 = reviews_neighbourhood[reviews_neighbourhood.year \n#                       >=2019].groupby('neighbourhood_group_cleansed')['price'].agg('mean').reset_index()\n\n# reviews_grouped_1 = reviews_grouped_1.merge(reviews_neighbourhood[['neighbourhood_group_cleansed', 'neighbourhood_cleansed']], \n#                                             left_on='neighbourhood_group_cleansed', right_on='neighbourhood_group_cleansed')\n\n# reviews_grouped_1 = reviews_grouped_1.drop_duplicates('neighbourhood_cleansed').reset_index(drop=True)\n\n# reviews_grouped_1['price_mean'] = reviews_grouped_1.price.apply(lambda x: round(x,2))\n\n# print(reviews_grouped_1.head(), reviews_grouped_1.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:01.51322Z","iopub.execute_input":"2022-02-22T19:33:01.513508Z","iopub.status.idle":"2022-02-22T19:33:02.529472Z","shell.execute_reply.started":"2022-02-22T19:33:01.513477Z","shell.execute_reply":"2022-02-22T19:33:02.528412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listings_df.neighbourhood_cleansed.unique(), listings_df.neighbourhood_group_cleansed.unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:02.530996Z","iopub.execute_input":"2022-02-22T19:33:02.531328Z","iopub.status.idle":"2022-02-22T19:33:02.535796Z","shell.execute_reply.started":"2022-02-22T19:33:02.531294Z","shell.execute_reply":"2022-02-22T19:33:02.534751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make the colormap used for the folium plot of neighbourhood prices\ncolormap = linear.OrRd_09.scale(\n    reviews_grouped.price_mean.min(),\n    reviews_grouped.price_mean.max())\n# make a dataframe that combines the geopandas df with the reviews_grouped dataframe\n\ngdf_price = geo_df.merge(reviews_grouped, left_on='neighbourhood', \n                               right_on = 'neighbourhood_cleansed')\n\n# tmp_price = gdf_price.groupby('neighbourhood_group')['geometry'].agg(list).reset_index()\n# from shapely.ops import unary_union\n# tmp_price['geometry'] = tmp_price['geometry'].apply(lambda x: unary_union(x))\n# tmp = gdf_price[['neighbourhood', 'neighbourhood_group', 'neighbourhood_group_cleansed', 'price', 'neighbourhood_cleansed', 'price_mean']].merge(tmp_price[['neighbourhood_group', 'geometry']], \n#                       left_on='neighbourhood_group', right_on='neighbourhood_group')\n\n# make map of average prices paid in each neighbourhood of Berlin\n\nstyle_function = lambda x: {'weight': 0.5,\n                           'fillColor': '#8c8c8c',\n                           'fillOpacity': 0,\n                           'color': 'black'} if pd.isnull(x['properties']['price_mean'])else {'weight': 0.5,\n                                                                                             'fillColor': colormap(x['properties']['price_mean']),\n                                                                                             'fillOpacity': .90,\n                                                                                             'color': 'black'}\n\nhighlight_function = lambda x: {'weight': 0.5,\n                           'fillColor': '#8c8c8c',\n                           'fillOpacity': 0,\n                           'color': 'black'} if pd.isnull(x['properties']['price_mean']) else {'weight': 0.9,\n                                                                                               'fillColor': colormap(x['properties']['price_mean']),\n                                                                                               'fillOpacity': 1,\n                                                                                               'color': 'black'}\n\nstyles= folium.features.GeoJson(\n    gdf_price,\n    style_function=style_function,\n    highlight_function = highlight_function,\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=['neighbourhood_cleansed','price_mean'],\n        aliases = ['Neighbourhood: ','Average Price Paid: ']))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:02.537092Z","iopub.execute_input":"2022-02-22T19:33:02.537468Z","iopub.status.idle":"2022-02-22T19:33:02.777281Z","shell.execute_reply.started":"2022-02-22T19:33:02.537435Z","shell.execute_reply":"2022-02-22T19:33:02.776392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results/ Visualisations:","metadata":{}},{"cell_type":"code","source":"# make folium map centered in Berlin\nm = folium.Map(location=[52.51, 13.40], zoom_start=11, tiles='cartodbpositron')\n\ncolormap.caption = 'Average Price Paid'\ncolormap.add_to(m)\n\nm.add_child(styles)\n\n# Save the map\nm.save(\"avg_price_paid.html\")\n\nm","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:02.779012Z","iopub.execute_input":"2022-02-22T19:33:02.779342Z","iopub.status.idle":"2022-02-22T19:33:03.641559Z","shell.execute_reply.started":"2022-02-22T19:33:02.779303Z","shell.execute_reply":"2022-02-22T19:33:03.640736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdf_price","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:03.643033Z","iopub.execute_input":"2022-02-22T19:33:03.643429Z","iopub.status.idle":"2022-02-22T19:33:03.70591Z","shell.execute_reply.started":"2022-02-22T19:33:03.643391Z","shell.execute_reply":"2022-02-22T19:33:03.704808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## District Map of Berlin","metadata":{}},{"cell_type":"code","source":"# # FOR THE neighbourhood_grouped_cleansed\n\n# reviews_grouped_1 = reviews_neighbourhood[reviews_neighbourhood.year \n#                       >=2019].groupby('neighbourhood_group_cleansed')['price'].agg('mean').reset_index()\n\n# reviews_grouped_1 = reviews_grouped_1.merge(reviews_neighbourhood[['neighbourhood_group_cleansed', 'neighbourhood_cleansed']], \n#                                             left_on='neighbourhood_group_cleansed', right_on='neighbourhood_group_cleansed')\n\n# reviews_grouped_1 = reviews_grouped_1.drop_duplicates('neighbourhood_cleansed').reset_index(drop=True)\n\n# reviews_grouped_1['price_mean'] = reviews_grouped_1.price.apply(lambda x: round(x,2))\n\n# print(reviews_grouped_1.head(), reviews_grouped_1.shape)\n\n# list_group = reviews_grouped_1['neighbourhood_group_cleansed'].unique().tolist()\n# reviews_grouped_1['number'] = reviews_grouped_1['neighbourhood_group_cleansed'].apply(lambda x: list_group.index(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:03.707265Z","iopub.execute_input":"2022-02-22T19:33:03.707493Z","iopub.status.idle":"2022-02-22T19:33:03.712611Z","shell.execute_reply.started":"2022-02-22T19:33:03.707465Z","shell.execute_reply":"2022-02-22T19:33:03.711635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdf_price = geo_df.merge(reviews_grouped, left_on='neighbourhood', \n                               right_on = 'neighbourhood_cleansed')\n\nplt.rcParams['figure.figsize'] = [20, 13]\nfig, ax = plt.subplots()\n\ngdf_price.plot(\n    ax=ax, \n    alpha=0.2\n)\n\ngdf_price.plot(\n    ax=ax, \n    column='neighbourhood_group',\n    categorical=True, \n    legend=True, \n    legend_kwds={'title': 'Neighbourhood', 'loc': 'upper right'},\n    cmap='tab20', \n    edgecolor='black',\n    \n)\n\nax.set(\n    title='Berlin District/Kiez', \n    aspect=1.3\n);\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:03.714233Z","iopub.execute_input":"2022-02-22T19:33:03.714555Z","iopub.status.idle":"2022-02-22T19:33:04.479728Z","shell.execute_reply.started":"2022-02-22T19:33:03.714514Z","shell.execute_reply":"2022-02-22T19:33:04.478646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdf_price.to_file('dataframe.geojson', driver='GeoJSON')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:04.481874Z","iopub.execute_input":"2022-02-22T19:33:04.482188Z","iopub.status.idle":"2022-02-22T19:33:04.915536Z","shell.execute_reply.started":"2022-02-22T19:33:04.482145Z","shell.execute_reply":"2022-02-22T19:33:04.914472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Look at prices in larger districts:","metadata":{}},{"cell_type":"code","source":"# get only information for those places stayed in 2019 and later\nreviews_boxplots = reviews_neighbourhood[reviews_neighbourhood.year >= \n                                        2019]\n\n# get all individual listing data for places with at least 1 review\nreviews_boxplots = reviews_boxplots.groupby('listing_id').first()\n\n# plot results as boxplots:\nfig = px.box(reviews_boxplots, x = 'neighbourhood_group_cleansed', y = 'price')\nfig.update_layout(\n    title=\"Listing Prices in Berlin Districts\",\n    yaxis_title=\"Listing Price (per night)\",\n    xaxis_title = 'District')\nfig.update_yaxes(range=[-30,1030])\nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Listing_Prices_in_Berlin_Districts.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:04.917535Z","iopub.execute_input":"2022-02-22T19:33:04.917788Z","iopub.status.idle":"2022-02-22T19:33:05.746842Z","shell.execute_reply.started":"2022-02-22T19:33:04.917759Z","shell.execute_reply":"2022-02-22T19:33:05.746195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get table of median and mean price information for each district as well:\ngroups = reviews_boxplots.groupby('neighbourhood_group_cleansed')['price'].agg(['mean',\n                                                                                'median']).sort_values(by='mean',\n                                                                                                       ascending=False).reset_index()\ngroups.columns = ['District','Mean Price','Median Price']\ngroups","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:05.747802Z","iopub.execute_input":"2022-02-22T19:33:05.748015Z","iopub.status.idle":"2022-02-22T19:33:05.768552Z","shell.execute_reply.started":"2022-02-22T19:33:05.747989Z","shell.execute_reply":"2022-02-22T19:33:05.767869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now, taking location ratings into account:\n### Data Preparation:","metadata":{}},{"cell_type":"code","source":"reviewed_listings_df = reviews_df[['listing_id',\n                                    'comments',\n                                    'date',\n                                   'year',\n                                   'month']].copy().merge(listings_df[['id',\n                                                                       'neighbourhood_group_cleansed',\n                                                                       'neighbourhood_cleansed',\n                                                                       'review_scores_location',\n                                                                       'price',\n                                                                       'latitude',\n                                                                       'longitude']].copy(),\n                                                          left_on='listing_id',\n                                                          right_on='id',\n                                                          how='inner').drop(columns = 'id')\n\nreviews_listings_df = reviewed_listings_df.groupby('listing_id').first()\n \n# make dataframes for review and price averages for each district and combine:\n\n\nlocation_reviews_df = reviewed_listings_df.groupby('neighbourhood_group_cleansed')[['review_scores_location']].mean()\nlocation_prices_df = reviewed_listings_df.groupby('neighbourhood_group_cleansed')[['price']].mean()\nlocation_reviews = pd.concat([location_reviews_df, location_prices_df], axis=1)\nlocation_reviews.reset_index(inplace=True)\nlocation_reviews.sort_values(by='review_scores_location', ascending=False, inplace = True)\nlocation_reviews.rename(columns = {'review_scores_location': 'Location Rating'}, inplace = True)\nlocation_reviews = location_reviews.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:05.773334Z","iopub.execute_input":"2022-02-22T19:33:05.774143Z","iopub.status.idle":"2022-02-22T19:33:06.59202Z","shell.execute_reply.started":"2022-02-22T19:33:05.774109Z","shell.execute_reply":"2022-02-22T19:33:06.590949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# location_reviews = location_reviews.reset_index(drop=True)\n# location_reviews.to_csv('location_reviews.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:06.593509Z","iopub.execute_input":"2022-02-22T19:33:06.593888Z","iopub.status.idle":"2022-02-22T19:33:06.598644Z","shell.execute_reply.started":"2022-02-22T19:33:06.593833Z","shell.execute_reply":"2022-02-22T19:33:06.597667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results/Visualisation:","metadata":{}},{"cell_type":"code","source":"fig = px.bar(location_reviews, x = 'neighbourhood_group_cleansed', \n             y = 'price', color = 'Location Rating')\nfig.update_layout(\n    title=\"Average District Price Relative to Location Rating\",\n    xaxis_title = \"District\",\n    yaxis_title = \"Price per Night ($)\"\n    )\nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Average_District_Price_Relative_to_Location_Rating.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:06.60005Z","iopub.execute_input":"2022-02-22T19:33:06.600381Z","iopub.status.idle":"2022-02-22T19:33:06.730559Z","shell.execute_reply.started":"2022-02-22T19:33:06.600346Z","shell.execute_reply":"2022-02-22T19:33:06.729846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap of reviews in time (not in post):","metadata":{}},{"cell_type":"code","source":"reviews_neighbourhood[['latitude','longitude','date','year','month']]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:06.732122Z","iopub.execute_input":"2022-02-22T19:33:06.732561Z","iopub.status.idle":"2022-02-22T19:33:06.756514Z","shell.execute_reply.started":"2022-02-22T19:33:06.732526Z","shell.execute_reply":"2022-02-22T19:33:06.755518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a dataframe of reviews for use in the heatmap\n\nheatmap_df = reviews_neighbourhood[['latitude','longitude','date','year','month']].copy()\nheatmap_df['lat_lon'] = heatmap_df.apply(lambda x: [round(x['latitude'], 7), round(x['longitude'],7)], axis=1)\n\n# prepare lat/lon list of lists for folium\nlat_lon_multistep = list(heatmap_df[(heatmap_df.year < 2016)].groupby(['year','month'])['lat_lon'].apply(list))\n# get year for each step to label the map with\ntimestamps = heatmap_df[heatmap_df.year < 2016].groupby(['year',\n                                                         'month'])[['date']].first().reset_index()\ntimestamps = list(timestamps.year)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:06.758088Z","iopub.execute_input":"2022-02-22T19:33:06.758304Z","iopub.status.idle":"2022-02-22T19:33:23.519852Z","shell.execute_reply.started":"2022-02-22T19:33:06.758277Z","shell.execute_reply":"2022-02-22T19:33:23.518671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results / visualisation:\n","metadata":{}},{"cell_type":"code","source":"m = folium.Map(location=[52.51, 13.40], zoom_start=11, tiles='cartodbpositron')\nheatmap = HeatMapWithTime(data = lat_lon_multistep,\n                              index=timestamps, auto_play=True, min_speed = 6)\nheatmap.add_to(m)\n\n\n# Save the map\nm.save(\"growth_of_aribnb.html\")\nm","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:23.52119Z","iopub.execute_input":"2022-02-22T19:33:23.521429Z","iopub.status.idle":"2022-02-22T19:33:24.655201Z","shell.execute_reply.started":"2022-02-22T19:33:23.521397Z","shell.execute_reply":"2022-02-22T19:33:24.654081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 3:\nCan we predict the location rating for a listing, and what neighbourhoods have the greatest influence on those ratings?\n\n## Data Preparation:","metadata":{}},{"cell_type":"code","source":"listings_df.amenities.unique(), listings_df.host_acceptance_rate.unique(), listings_df.review_scores_location.unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:24.657008Z","iopub.execute_input":"2022-02-22T19:33:24.657249Z","iopub.status.idle":"2022-02-22T19:33:24.69514Z","shell.execute_reply.started":"2022-02-22T19:33:24.657219Z","shell.execute_reply":"2022-02-22T19:33:24.694465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the review scores column names:\ntarget_colnames = ['review_scores_rating',\n       'review_scores_accuracy', 'review_scores_cleanliness',\n       'review_scores_checkin', 'review_scores_communication',\n       'review_scores_location', 'review_scores_value']\ndf = listings_df.copy()\n# clean variables that are strings but should be floats:\n\ndf['host_response_rate'] = df.host_response_rate.apply(\n    lambda x: x if pd.isnull(x) else float(x.strip('%')))\n\ndf['host_acceptance_rate'] = df.host_acceptance_rate.apply(\n    lambda x: x if pd.isnull(x) else float(x.strip('%')))\n\n# change date of host_since to num days they have been a host\ndf['host_days'] = df.host_since.apply(lambda x: (pd.to_datetime('today') - pd.to_datetime(x)).days)\ndf.drop('host_since', 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:24.696626Z","iopub.execute_input":"2022-02-22T19:33:24.697091Z","iopub.status.idle":"2022-02-22T19:33:30.08152Z","shell.execute_reply.started":"2022-02-22T19:33:24.697058Z","shell.execute_reply":"2022-02-22T19:33:30.080205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop all rows that have NA values in the target variable we are using (they all have a similar number of NA values)\ndf.dropna(subset = ['review_scores_location'], inplace = True)\n\n# make a feature for how many cancellations each listing has had\nreviews_df['canceled'] = reviews_df.comments.str.contains('canceled|cancellation')\ncanceled_df = reviews_df.groupby('listing_id')[['canceled']].sum().astype(int)\ndf = df.merge(canceled_df, left_on='id', right_on='listing_id')\n# More feature engineering:\n# print(df.head())\n\n# make features for the number of verifications a host has, the number of amenities, and clean\n# the amenities column\ndf['num_verifications'] = df.host_verifications.apply(\n    lambda x: len(x.strip('][').replace(\"'\",\"\").split(', ')))\ndf['amenities'] = df.amenities.apply(\n    lambda x: x.strip('}{').replace(\"'\",\"\").replace(\"\\\"\", \"\").split(','))\ndf['num_amenities'] = df.amenities.apply(\n    lambda x: len(x))\n\n# feature for how many missing values a listing has\ndf['num_null'] = df.isnull().sum(axis=1)\n# print(df.head())\n# clean all columns with prices\nprice_cols = ['weekly_price','security_deposit','cleaning_fee', 'extra_people']\n\ndf.loc[:,price_cols] = df[price_cols].applymap(\n    lambda x: x if pd.isnull(x) else float(x.replace(\"$\", \"\").replace(\",\",\"\")))\n# get the distance in km that each listing is from the city centre\n\ncity_centre_coords = (13.3888599, 52.5170365)\ndf['dist_to_center'] = df.apply(\n    lambda x: geopy.distance.geodesic(city_centre_coords, (x['longitude'],\n                                                           x['latitude'])).km, 1)\n# print(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:30.083285Z","iopub.execute_input":"2022-02-22T19:33:30.08354Z","iopub.status.idle":"2022-02-22T19:33:38.339939Z","shell.execute_reply.started":"2022-02-22T19:33:30.083509Z","shell.execute_reply":"2022-02-22T19:33:38.339015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Geographical Feature Engineering:\nGet extra geographical data from OSM, first:\n\n### Distances of properties to closest train station/underground:","metadata":{}},{"cell_type":"code","source":"# get the latitude and longitude values for all U-Bahn and S-bahn stations in Berlin\noverpass_url = \"http://overpass-api.de/api/interpreter\"\noverpass_query = \"\"\"\n[out:json];\narea[name = \"Berlin\"];\n(node(area)[railway=station];\nnode(area)[station=subway];\n);out center;\n\"\"\"\nresponse = requests.get(overpass_url, \n                        params={'data': overpass_query})\ndata = response.json()\n# Collect coords into list and plot:\ncoords =[]\nlon_ls = []\nlat_ls = []\ntype_ls = []\nname_ls = []\nfor element in data['elements']:\n    if element['type'] == 'node':\n        lon = element['lon']\n        lat = element['lat']\n        station_Sbahn= element['tags']['name'].startswith('S') # startswith() returns True or False\n        name = element['tags']['name']\n        if lon >=12:\n            lon_ls.append(lon)\n            lat_ls.append(lat)\n            coords.append((lon, lat))\n            type_ls.append(station_Sbahn)\n            name_ls.append(name)\n# Convert coordinates into numpy array\nX = np.array(coords)\nplt.plot(X[:, 0], X[:, 1], 'o')\nplt.title('Stations in Berlin')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.axis('equal')\nplt.show()\n\n# make data frame of the latitude and longitudes:\ncoord_df = pd.DataFrame({'latitude_value': lat_ls, \n                         'longitude_value': lon_ls, 'sbahn': type_ls})","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:38.341263Z","iopub.execute_input":"2022-02-22T19:33:38.34151Z","iopub.status.idle":"2022-02-22T19:33:43.265188Z","shell.execute_reply.started":"2022-02-22T19:33:38.341479Z","shell.execute_reply":"2022-02-22T19:33:43.264335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loc = 'Station Locations'\ntitle_html = '''\n             <h3 align=\"center\" style=\"font-size:16px\"><b>{}</b></h3>\n             '''.format(loc)   \n\nstat_df = gpd.GeoDataFrame(coord_df, geometry=gpd.points_from_xy(coord_df.latitude_value, coord_df.longitude_value))\nm = folium.Map(location=[52.51, 13.40], zoom_start=10, tiles='cartodbpositron')\nfor point in range(0, len(stat_df)):\n    folium.CircleMarker([stat_df.loc[point, 'latitude_value'], stat_df.loc[point, 'longitude_value']], radius=3).add_to(m)\n\nstyle_function = lambda x: {'weight': 0.5,\n                           'fillColor': '#8c8c8c',\n                           'fillOpacity': 0,\n                           'color': 'black'} \n\nstyles= folium.features.GeoJson(\n    gdf_price,\n    style_function=style_function)\n\nm.add_child(styles)\nm.get_root().html.add_child(folium.Element(title_html))\n\n# Save the map\nm.save(\"station_locations.html\")\n\nm","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:43.266635Z","iopub.execute_input":"2022-02-22T19:33:43.266876Z","iopub.status.idle":"2022-02-22T19:33:44.717973Z","shell.execute_reply.started":"2022-02-22T19:33:43.266846Z","shell.execute_reply":"2022-02-22T19:33:44.716701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coord_df.to_csv('station.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:44.719716Z","iopub.execute_input":"2022-02-22T19:33:44.719972Z","iopub.status.idle":"2022-02-22T19:33:44.763617Z","shell.execute_reply.started":"2022-02-22T19:33:44.719941Z","shell.execute_reply":"2022-02-22T19:33:44.76266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all pairwise distances between each listing and stations\nimport scipy\ndistances = scipy.spatial.distance.cdist(df[['latitude','longitude']], \n                                         coord_df[['latitude_value','longitude_value']],\n                                         metric='seuclidean')\n\ndistance_sbahn = scipy.spatial.distance.cdist(df[['latitude','longitude']], \n                                              coord_df[coord_df.sbahn][['latitude_value','longitude_value']],\n                                             metric='seuclidean')\n\ndistance_ubahn = scipy.spatial.distance.cdist(df[['latitude','longitude']], \n                                              coord_df[coord_df.sbahn == False][['latitude_value',\n                                                                                 'longitude_value']],\n                                              metric='seuclidean')\n# make feature for the distance to closest train station\ndf['dist_to_station'] = distances.min(axis=1)\ndf['dist_to_sbahn'] = distance_sbahn.min(axis=1)\ndf['dist_to_ubahn'] = distance_ubahn.min(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:44.765192Z","iopub.execute_input":"2022-02-22T19:33:44.765658Z","iopub.status.idle":"2022-02-22T19:33:44.943021Z","shell.execute_reply.started":"2022-02-22T19:33:44.765584Z","shell.execute_reply":"2022-02-22T19:33:44.941939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distances[0].min(), distance_sbahn[0].min(), distance_ubahn[0].min()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:44.944524Z","iopub.execute_input":"2022-02-22T19:33:44.944931Z","iopub.status.idle":"2022-02-22T19:33:44.954353Z","shell.execute_reply.started":"2022-02-22T19:33:44.944884Z","shell.execute_reply":"2022-02-22T19:33:44.953153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distance to closest Biergarten: (A beer garden (German: Biergarten) is an outdoor area in which beer and food are served, typically at shared tables.)","metadata":{}},{"cell_type":"code","source":"overpass_url = \"http://overpass-api.de/api/interpreter\"\noverpass_query = \"\"\"\n[out:json];\narea[name = \"Berlin\"];\n(node(area)[\"amenity\"=\"biergarten\"];\n);out center;\n\"\"\"\nresponse = requests.get(overpass_url, \n                        params={'data': overpass_query})\ndata = response.json()\n# Collect coords into list and plot:\nlon_ls = []\nlat_ls = []\nfor element in data['elements']:\n    if element['type'] == 'node':\n        lon = element['lon']\n        lat = element['lat']\n        if lon >=12:\n            lon_ls.append(lon)\n            lat_ls.append(lat)\n# make data frame of the latitude and longitudes:\ncoord_df = pd.DataFrame({'latitude_value': lat_ls, 'longitude_value': lon_ls})\n# get all pairwise distances between each listing and beer gardens\ndistances = scipy.spatial.distance.cdist(df[['latitude','longitude']], \n                                         coord_df[['latitude_value','longitude_value']],\n                                         metric='seuclidean')\ndf['dist_to_biergarten'] = distances.min(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:44.956142Z","iopub.execute_input":"2022-02-22T19:33:44.956684Z","iopub.status.idle":"2022-02-22T19:33:49.961426Z","shell.execute_reply.started":"2022-02-22T19:33:44.956647Z","shell.execute_reply":"2022-02-22T19:33:49.960492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coord_df","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:49.962736Z","iopub.execute_input":"2022-02-22T19:33:49.962973Z","iopub.status.idle":"2022-02-22T19:33:49.967635Z","shell.execute_reply.started":"2022-02-22T19:33:49.962942Z","shell.execute_reply":"2022-02-22T19:33:49.966585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loc = 'Biergarten Locations'\ntitle_html = '''\n             <h3 align=\"center\" style=\"font-size:16px\"><b>{}</b></h3>\n             '''.format(loc) \n\nstat_df = gpd.GeoDataFrame(coord_df, geometry=gpd.points_from_xy(coord_df.latitude_value, coord_df.longitude_value))\nm = folium.Map(location=[52.51, 13.40], zoom_start=11, tiles='cartodbpositron')\nfor point in range(0, len(stat_df)):\n    folium.CircleMarker([stat_df.loc[point, 'latitude_value'], stat_df.loc[point, 'longitude_value']], radius=3, color = 'green').add_to(m)\n\nstyle_function = lambda x: {'weight': 0.5,\n                           'fillColor': '#8c8c8c',\n                           'fillOpacity': 0,\n                           'color': 'black'} \n\nstyles= folium.features.GeoJson(\n    gdf_price,\n    style_function=style_function)\n\nm.add_child(styles)\nm.get_root().html.add_child(folium.Element(title_html))\n\n# Save the map\nm.save(\"biergarten.html\")\n\nm","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:49.968981Z","iopub.execute_input":"2022-02-22T19:33:49.969341Z","iopub.status.idle":"2022-02-22T19:33:51.084604Z","shell.execute_reply.started":"2022-02-22T19:33:49.969292Z","shell.execute_reply":"2022-02-22T19:33:51.083505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coord_df.to_csv('biergarten.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:51.08633Z","iopub.execute_input":"2022-02-22T19:33:51.08666Z","iopub.status.idle":"2022-02-22T19:33:51.106709Z","shell.execute_reply.started":"2022-02-22T19:33:51.086622Z","shell.execute_reply":"2022-02-22T19:33:51.105668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list(listings_df.columns[listings_df.isnull().sum()/listings_df.shape[0] >= .95])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:51.108405Z","iopub.execute_input":"2022-02-22T19:33:51.109059Z","iopub.status.idle":"2022-02-22T19:33:51.113666Z","shell.execute_reply.started":"2022-02-22T19:33:51.109011Z","shell.execute_reply":"2022-02-22T19:33:51.11271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numeric variables:","metadata":{}},{"cell_type":"code","source":"# make a dataframe with only the numeric variables in df\nnum_vars = df.select_dtypes(exclude=['object'])\n\n# get a list of cols to drop, including those that are missing more than 95% of the values, as well as the ID variables\n# and lat, lon which don't have relevance to this prediction\n\ncols_to_drop = list(num_vars.columns[num_vars.isnull().sum()/num_vars.shape[0] >= .95]) + ['id',\n                                                                                           'scrape_id',\n                                                                                           'host_id',\n                                                                                           'latitude',\n                                                                                           'longitude',\n                                                                                           'host_listings_count',\n                                                                                           'host_total_listings_count']\n# drop the cols_to_drop columns:\nnum_vars = num_vars.drop(columns = cols_to_drop)\n# make histograms\n# change the iloc argument [:,:9] to whichever variables you want to look at\n# (not including all histograms for lack of space)\n\nnum_vars.iloc[:,:9].hist(figsize=(15,8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:51.115139Z","iopub.execute_input":"2022-02-22T19:33:51.115584Z","iopub.status.idle":"2022-02-22T19:33:52.630255Z","shell.execute_reply.started":"2022-02-22T19:33:51.115552Z","shell.execute_reply":"2022-02-22T19:33:52.629382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make features for certain columns which we don't want to impute, but still want to know which values are null\n# in case they are significant\n\nnum_vars['weekly_price_null'] = num_vars.weekly_price.isnull()\nnum_vars.drop('weekly_price', 1, inplace = True)\n\nnum_vars['deposit_null'] = num_vars.security_deposit.isnull()\nnum_vars['cleaning_fee_null'] = num_vars.cleaning_fee.isnull()\n\nnum_vars.drop('cleaning_fee',1, inplace = True)\n\n# make variables for acceptance and response rate null values, as they are probably important, and we don't\n# want to throw out that information\n\nnum_vars['host_acceptance_rate_null'] = num_vars.host_acceptance_rate.apply(\n    lambda x: pd.isnull(x))\nnum_vars['host_response_rate_null'] = num_vars.host_response_rate.apply(\n    lambda x: pd.isnull(x))\n# IMPUTATION: Not using currently as we are only interested in the location columns. However,\n# if we would like to predict other ratings, can use these imputation methods\n\n# impute the following columns to median value\nimpute_median_cols = ['bedrooms','beds','bathrooms',\n                      'reviews_per_month','security_deposit','host_days',\n                     'price']\nnum_vars.loc[:,impute_median_cols] = num_vars[impute_median_cols].fillna(num_vars[impute_median_cols].median())\n\n# # now impute the columns to the mean or median value\nnum_vars['host_acceptance_rate'] = num_vars.host_acceptance_rate.fillna(num_vars.host_acceptance_rate.mean())\nnum_vars['host_response_rate'] = num_vars.host_response_rate.fillna(num_vars.host_response_rate.mean())\nnum_vars['host_days'] = num_vars.host_days.fillna(num_vars.host_days.median())\n# from stackoverflow: https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=30):\n    '''Get the top correlations wrt absolute value'''\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n# look at the top correlations in the dataset so we can remove redundant variables\ncorr_df = num_vars.corr()\nget_top_abs_correlations(corr_df).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:52.631556Z","iopub.execute_input":"2022-02-22T19:33:52.631826Z","iopub.status.idle":"2022-02-22T19:33:52.896909Z","shell.execute_reply.started":"2022-02-22T19:33:52.631795Z","shell.execute_reply":"2022-02-22T19:33:52.895679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the following columns as they are too highly correlated with other columns\nredundant_cols = ['bathrooms', 'accommodates',\n                  'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm',\n                 'minimum_minimum_nights','maximum_minimum_nights',\n                  'maximum_maximum_nights',\n                 'minimum_maximum_nights', 'availability_60', 'availability_90',\n                  'number_of_reviews_ltm']\n\nnum_vars.drop(columns = redundant_cols, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:52.898717Z","iopub.execute_input":"2022-02-22T19:33:52.899271Z","iopub.status.idle":"2022-02-22T19:33:52.908122Z","shell.execute_reply.started":"2022-02-22T19:33:52.899221Z","shell.execute_reply":"2022-02-22T19:33:52.906962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars.columns, num_vars.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:52.909445Z","iopub.execute_input":"2022-02-22T19:33:52.909751Z","iopub.status.idle":"2022-02-22T19:33:52.928159Z","shell.execute_reply.started":"2022-02-22T19:33:52.909719Z","shell.execute_reply":"2022-02-22T19:33:52.927418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import preprocessing\n# min_max_scaler = preprocessing.MinMaxScaler()\n# cols = ['host_response_rate', 'host_acceptance_rate', 'bedrooms', 'beds',\n#        'price', 'security_deposit', 'guests_included', 'extra_people',\n#        'minimum_nights', 'maximum_nights', 'availability_30',\n#        'availability_365', 'number_of_reviews', 'review_scores_rating',\n#        'review_scores_accuracy', 'review_scores_cleanliness',\n#        'review_scores_checkin', 'review_scores_communication', 'review_scores_value',\n#        'calculated_host_listings_count',\n#        'calculated_host_listings_count_entire_homes',\n#        'calculated_host_listings_count_private_rooms',\n#        'calculated_host_listings_count_shared_rooms', 'reviews_per_month',\n#        'host_days', 'canceled', 'num_verifications', 'num_amenities',\n#        'num_null', 'dist_to_center', 'dist_to_station', 'dist_to_sbahn',\n#        'dist_to_ubahn', 'dist_to_biergarten', 'weekly_price_null',\n#        'deposit_null', 'cleaning_fee_null', 'host_acceptance_rate_null',\n#        'host_response_rate_null']\n# num_vars[cols] = min_max_scaler.fit_transform(num_vars[cols])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:52.929744Z","iopub.execute_input":"2022-02-22T19:33:52.929977Z","iopub.status.idle":"2022-02-22T19:33:52.939444Z","shell.execute_reply.started":"2022-02-22T19:33:52.929949Z","shell.execute_reply":"2022-02-22T19:33:52.938484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars.review_scores_location","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:52.941484Z","iopub.execute_input":"2022-02-22T19:33:52.941781Z","iopub.status.idle":"2022-02-22T19:33:52.956058Z","shell.execute_reply.started":"2022-02-22T19:33:52.941739Z","shell.execute_reply":"2022-02-22T19:33:52.954906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Variables:","metadata":{}},{"cell_type":"code","source":"# get a dataframe of just the categorical variables\ncat_vars = df.select_dtypes(include=['object']).copy()\n\n# take only columns of interest\ncat_vars = cat_vars[['host_response_time',#'host_neighbourhood','host_location',\n         'host_is_superhost','host_has_profile_pic','host_identity_verified',\n          'neighbourhood_cleansed','neighbourhood_group_cleansed',\n                    'property_type','room_type','bed_type','requires_license',\n               'instant_bookable','cancellation_policy','amenities']]\n# Clean up the amenities column, and make dummy variables for the important amenities\n\namenity_list_list = cat_vars.amenities.to_list()\namenity_list = [item for sublist in amenity_list_list for item in sublist]\n# get the counts of each amenity by how many times it appears\ncounts = pd.Series(amenity_list).value_counts()\nvalues = cat_vars.amenities.copy()\n\n# get a series of the top 20 advertised amenities\ntop_amenities = pd.Series(counts.iloc[:20].index)\n# set all listings that have any of those top 20 amenities to true (to make the mask)\nmask = pd.Series([(top_amenities.any() in value) for value in values])\n# mask out amenities that are not in the top 20 most popular\nvalues[~mask] = 'other'\n# add a dummy column for each of the top 20 amenities\nfor col in top_amenities:\n    values = [col in amenity_ls for amenity_ls in cat_vars.amenities] # returns bool\n    cat_vars[col] = values\n    \n# drop amenities column\ncat_vars.drop(columns = 'amenities', inplace = True)\n# print(cat_vars.head())\n# make dummy variables for neighbourhoods (only those with at least 200 counts, otherwise there are too many)\n\nvalues = cat_vars.neighbourhood_cleansed.copy()\ncounts = cat_vars.neighbourhood_cleansed.value_counts()\nmask = values.isin(counts[counts > 200].index)\n# print(mask[:40])\nvalues[~mask] = \"other\"\n# make dummy columns from the top neighbourhoods:\ndummies = pd.get_dummies(values)\ndummies.columns = pd.Series(dummies.columns).apply(lambda x: 'neighbourhood_cleansed_' + x)\ndummies.drop(columns = 'neighbourhood_cleansed_other', inplace = True)\n\n# add dummy variables back into cat_vars df\ncat_vars = pd.concat([cat_vars.drop('neighbourhood_cleansed', axis=1), dummies], 1)\n# get dummies for other important variables:\n\ncat_vars = pd.get_dummies(cat_vars, \n                          columns = ['host_response_time','neighbourhood_group_cleansed',\n                                    'room_type','bed_type', 'cancellation_policy'],\n                          drop_first = True)\n# change variables that just have a t or f to bools\ntf_vars = ['requires_license','instant_bookable',\n         'host_is_superhost','host_has_profile_pic','host_identity_verified']\ncat_vars.loc[:,tf_vars] = cat_vars[tf_vars].applymap(lambda x: True if x=='t' else False)\n# make dummy variables for the most common property types\n\nvalues = cat_vars.property_type.copy()\ncounts = cat_vars.property_type.value_counts()\nmask = values.isin(counts[counts > 3].index)\nvalues[~mask] = \"other\"\n\ndummies = pd.get_dummies(values)\ndummies.columns = pd.Series(dummies.columns).apply(lambda x: 'property_type_' + x)\ndummies.drop(columns = 'property_type_other', inplace = True)\n\ncat_vars = pd.concat([cat_vars.drop('property_type', axis=1), dummies], 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:52.959442Z","iopub.execute_input":"2022-02-22T19:33:52.959722Z","iopub.status.idle":"2022-02-22T19:33:56.632915Z","shell.execute_reply.started":"2022-02-22T19:33:52.959688Z","shell.execute_reply":"2022-02-22T19:33:56.631645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_vars.columns, cat_vars.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:56.634952Z","iopub.execute_input":"2022-02-22T19:33:56.635269Z","iopub.status.idle":"2022-02-22T19:33:56.643068Z","shell.execute_reply.started":"2022-02-22T19:33:56.635229Z","shell.execute_reply":"2022-02-22T19:33:56.642024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars.review_scores_value","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:56.6448Z","iopub.execute_input":"2022-02-22T19:33:56.645177Z","iopub.status.idle":"2022-02-22T19:33:56.659562Z","shell.execute_reply.started":"2022-02-22T19:33:56.645138Z","shell.execute_reply":"2022-02-22T19:33:56.65883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling:\n### Fit a logistic regression model:","metadata":{}},{"cell_type":"code","source":"def coef_weights(coefficients, X_train):\n    '''\n    Inputs:\n    coefficients - the coefficients of the linear model \n    X_train - the training data, so the column names can be used\n    Output:\n    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n    \n    Provides a dataframe that can be used to understand the most influential coefficients\n    in a linear model by providing the coefficient estimates along with the name of the \n    variable attached to the coefficient.\n    '''\n    coefs_df = pd.DataFrame()\n    coefs_df['est_int'] = X_train.columns\n    coefs_df['coefs'] = coefficients\n    coefs_df['abs_coefs'] = np.abs(coefficients)\n    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n    return coefs_df\n# reset indices so we can combine numeric and categorical variables\ncat_vars.reset_index(drop = True, inplace = True)\nnum_vars.reset_index(drop = True, inplace = True)\n\n# combine dataframes to get full dataframe\nfill_df = pd.concat([cat_vars, num_vars], axis = 1)\n\n# change the boolean values to integers\nbool_df = fill_df.select_dtypes(include='bool').astype(int)\nfill_df[bool_df.columns] = bool_df","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:56.661353Z","iopub.execute_input":"2022-02-22T19:33:56.661844Z","iopub.status.idle":"2022-02-22T19:33:56.703231Z","shell.execute_reply.started":"2022-02-22T19:33:56.661806Z","shell.execute_reply":"2022-02-22T19:33:56.70221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Focus only on features relevant for predicting LOCATION ratings:\n(Can change this if you want to use other columns)","metadata":{}},{"cell_type":"code","source":"# location_cols = list(fill_df.columns[pd.Series(fill_df.columns).str.contains('location|neighbourhood|dist|room_type|property_type')])\n\n# # add any other features we want to include in model:\n# location_cols = location_cols + ['host_is_superhost','host_response_rate_null']\n# fill_df = fill_df[location_cols]\n# look at highly correlated columns so we can reduce multi-collinearity, problematic in linear models\n\ncorr_df = fill_df.corr()\nget_top_abs_correlations(corr_df, n = 11)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:56.704552Z","iopub.execute_input":"2022-02-22T19:33:56.704831Z","iopub.status.idle":"2022-02-22T19:33:58.113073Z","shell.execute_reply.started":"2022-02-22T19:33:56.704799Z","shell.execute_reply":"2022-02-22T19:33:58.112075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the columns that are too highly correlated with other columns in dataframe\n\nredundant_cols = ['neighbourhood_cleansed_Neu Lichtenberg', \n                  'neighbourhood_cleansed_Schneberg-Nord',\n                  'room_type_Hotel room','room_type_Shared room', 'dist_to_ubahn',\n                  'neighbourhood_cleansed_Schillerpromenade',\n                 'neighbourhood_cleansed_Schneberg-Sd',\n                  'neighbourhood_cleansed_Reuterstrae',\n                  'neighbourhood_cleansed_Alexanderplatz', 'neighbourhood_cleansed_Rixdorf',\n                 'neighbourhood_cleansed_Neukllner Mitte/Zentrum',\n                  'neighbourhood_cleansed_Frankfurter Allee Sd FK',\n                 'neighbourhood_cleansed_Tempelhof',\n                  'neighbourhood_cleansed_Tempelhofer Vorstadt',\n                 'neighbourhood_cleansed_Brunnenstr. Nord']\n\n\n\nfill_df = fill_df.drop(columns = redundant_cols)\n# examine distribution of target column:\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:58.11481Z","iopub.execute_input":"2022-02-22T19:33:58.115097Z","iopub.status.idle":"2022-02-22T19:33:58.133646Z","shell.execute_reply.started":"2022-02-22T19:33:58.115056Z","shell.execute_reply":"2022-02-22T19:33:58.132666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (8,6))\nax = fig.gca()\n\nfill_df['review_scores_value'].hist(ax = ax)\nplt.title('Histogram of Location Ratings')\nplt.ylabel('Count')\nplt.xlabel('Rating')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:58.135102Z","iopub.execute_input":"2022-02-22T19:33:58.135348Z","iopub.status.idle":"2022-02-22T19:33:58.363272Z","shell.execute_reply.started":"2022-02-22T19:33:58.135318Z","shell.execute_reply":"2022-02-22T19:33:58.362393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(fill_df, x = 'review_scores_value')\nfig.update_layout(\n    title=\"Histogram of Listing Rating\",\n    xaxis_title = \"Rating\",\n    yaxis_title = \"Count\"\n    )\nfig.show()\n\n# Saving the plot\nplotly.offline.plot(fig, filename='Histogram_of_Location_Rating.html')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:58.364891Z","iopub.execute_input":"2022-02-22T19:33:58.36518Z","iopub.status.idle":"2022-02-22T19:33:58.569895Z","shell.execute_reply.started":"2022-02-22T19:33:58.365147Z","shell.execute_reply":"2022-02-22T19:33:58.569197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:33:58.570805Z","iopub.execute_input":"2022-02-22T19:33:58.571029Z","iopub.status.idle":"2022-02-22T19:33:58.578013Z","shell.execute_reply.started":"2022-02-22T19:33:58.571003Z","shell.execute_reply":"2022-02-22T19:33:58.577057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\n# set X dataframe\nX = fill_df.drop(columns = ['review_scores_value'], axis = 1)\n# min_max_scaler = preprocessing.MinMaxScaler()\n# X[['dist_to_center', 'dist_to_station', 'dist_to_sbahn', 'dist_to_biergarten']] = min_max_scaler.fit_transform(X[['dist_to_center', 'dist_to_station', 'dist_to_sbahn', 'dist_to_biergarten']])\n\n# set target column, predicting either rating == 10 or not\ny = fill_df['review_scores_value']\ny = (y < 10).astype(int)\n# see that 70% of ratings = 10 and the other 30% are less than 10\ny.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:01.636328Z","iopub.execute_input":"2022-02-22T19:47:01.636642Z","iopub.status.idle":"2022-02-22T19:47:01.654454Z","shell.execute_reply.started":"2022-02-22T19:47:01.636612Z","shell.execute_reply":"2022-02-22T19:47:01.653408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X =X.fillna(X.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:01.8723Z","iopub.execute_input":"2022-02-22T19:47:01.87285Z","iopub.status.idle":"2022-02-22T19:47:01.913304Z","shell.execute_reply.started":"2022-02-22T19:47:01.872799Z","shell.execute_reply":"2022-02-22T19:47:01.912567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that there are no NA values\nprint('There are', X.isnull().sum().sum(), 'null values in the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:02.188226Z","iopub.execute_input":"2022-02-22T19:47:02.188828Z","iopub.status.idle":"2022-02-22T19:47:02.201996Z","shell.execute_reply.started":"2022-02-22T19:47:02.188792Z","shell.execute_reply":"2022-02-22T19:47:02.201067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:02.358102Z","iopub.execute_input":"2022-02-22T19:47:02.358953Z","iopub.status.idle":"2022-02-22T19:47:02.367738Z","shell.execute_reply.started":"2022-02-22T19:47:02.358894Z","shell.execute_reply":"2022-02-22T19:47:02.366395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_top_abs_correlations(X, n = 11)\nX = X.drop(['Cooking basics', 'Oven'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:03.917018Z","iopub.execute_input":"2022-02-22T19:47:03.918017Z","iopub.status.idle":"2022-02-22T19:47:05.019969Z","shell.execute_reply.started":"2022-02-22T19:47:03.917982Z","shell.execute_reply":"2022-02-22T19:47:05.019167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # split into train and test sets:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n# # # fit a logistic regression model, classifying whether rating is 10 or not\n# # # lr_model = sm.Logit(y_train, X_train).fit(inflation='probit')\n# import warnings\n\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import LogisticRegression\n\n# # pipe = Pipeline([('classifier' , RandomForestClassifier())])\n# # # pipe = Pipeline([('classifier', RandomForestClassifier())])\n\n# # # Create param grid.\n\n# # param_grid = [\n# #     {'classifier' : [LogisticRegression()],\n# #      'classifier__penalty' : ['l1', 'l2'],\n# #     'classifier__C' : np.logspace(-4, 4, 20),\n# #     'classifier__solver' : ['liblinear']},\n# #     {'classifier' : [RandomForestClassifier()],\n# #     'classifier__n_estimators' : list(range(10,101,10)),\n# #     'classifier__max_features' : list(range(6,32,5))}\n# # ]\n\n# # # Create grid search object\n\n# # lr_model = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True)\n\n\n\n# param_grid = {    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n#     'penalty': ['l1', 'l2']\n#              }\n# lr_model = GridSearchCV(LogisticRegression(random_state=0,solver='liblinear'), param_grid = param_grid, cv = 5, verbose=True)\n\n# lr_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:06.429885Z","iopub.execute_input":"2022-02-22T19:47:06.430676Z","iopub.status.idle":"2022-02-22T19:47:06.455903Z","shell.execute_reply.started":"2022-02-22T19:47:06.430632Z","shell.execute_reply":"2022-02-22T19:47:06.4549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = False","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:08.407123Z","iopub.execute_input":"2022-02-22T19:47:08.407439Z","iopub.status.idle":"2022-02-22T19:47:08.411674Z","shell.execute_reply.started":"2022-02-22T19:47:08.407402Z","shell.execute_reply":"2022-02-22T19:47:08.411004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:47:09.133563Z","iopub.execute_input":"2022-02-22T19:47:09.133871Z","iopub.status.idle":"2022-02-22T19:47:09.138404Z","shell.execute_reply.started":"2022-02-22T19:47:09.133839Z","shell.execute_reply":"2022-02-22T19:47:09.137544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nskf = StratifiedKFold(n_splits=5)\n\n\nif t==False:\n    X_tmp = X\n    y_tmp = y\n    t = True\nelse:\n    X= X_tmp\n    y = y_tmp\n\nfold_no=1\nlr_model = LogisticRegression(random_state=200,solver='newton-cg', penalty='l2', class_weight='balanced', \n                              max_iter=700, warm_start=True, multi_class='ovr', C=1)\nX, X_test, y, y_test = train_test_split(X, y, test_size = .30, random_state=42)\nfor train_index,test_index in skf.split(X, y):\n    if fold_no == 5: \n        continue\n    train = X.iloc[train_index,:]\n    test = X.iloc[test_index,:]\n    \n    lr_model.fit(train, y.iloc[train_index])\n    \n    y_pred_train_prob = lr_model.predict(train)\n    y_pred_test_prob = lr_model.predict(test)\n    # choose threshold of .5 for probability threshold to classify into 10 rating\n    y_pred_test = (y_pred_test_prob >= .5).astype(int)\n    y_pred_train = (y_pred_train_prob >= .5).astype(int)\n\n\n    print('Train Accuracy:', accuracy_score(y.iloc[train_index], y_pred_train))\n    print('Test Accuracy:', accuracy_score(y.iloc[test_index], y_pred_test))\n    \n    lr_precision, lr_recall, _ = precision_recall_curve(y.iloc[test_index], y_pred_test_prob)\n    lr_f1, lr_auc = f1_score(y.iloc[test_index], y_pred_test), auc(lr_recall, lr_precision)\n\n    # summarize scores\n    # note: a baseline AUC for precision-recall would be about .3 since 30% of the instances are class 1\n    print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n    fold_no += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:11:16.592747Z","iopub.execute_input":"2022-02-22T20:11:16.593064Z","iopub.status.idle":"2022-02-22T20:11:52.326922Z","shell.execute_reply.started":"2022-02-22T20:11:16.593033Z","shell.execute_reply":"2022-02-22T20:11:52.325962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get test and train predictions:\ny_pred_test_prob = lr_model.predict(X_test)\ny_pred_train_prob = lr_model.predict(X_train)\n# choose threshold of .5 for probability threshold to classify into 10 rating\ny_pred_test = (y_pred_test_prob >= .5).astype(int)\ny_pred_train = (y_pred_train_prob >= .5).astype(int)\n\n\nprint('Train Accuracy:', accuracy_score(y_train, y_pred_train))\nprint('Test Accuracy:', accuracy_score(y_test, y_pred_test))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:11:52.32931Z","iopub.execute_input":"2022-02-22T20:11:52.329905Z","iopub.status.idle":"2022-02-22T20:11:52.373668Z","shell.execute_reply.started":"2022-02-22T20:11:52.329856Z","shell.execute_reply":"2022-02-22T20:11:52.372336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy doesn't tell us much, since the classes are quite imbalanced, so let's look at precision\n# recall curves and f1 score\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, y_pred_test_prob)\nlr_f1, lr_auc = f1_score(y_test, y_pred_test), auc(lr_recall, lr_precision)\n\n# summarize scores\n# note: a baseline AUC for precision-recall would be about .3 since 30% of the instances are class 1\nprint('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n\n# Logistic: f1=0.726 auc=0.786","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:11:52.37618Z","iopub.execute_input":"2022-02-22T20:11:52.376977Z","iopub.status.idle":"2022-02-22T20:11:52.395202Z","shell.execute_reply.started":"2022-02-22T20:11:52.376921Z","shell.execute_reply":"2022-02-22T20:11:52.394215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n#Generate the confusion matrix\ncf_matrix = confusion_matrix(y_test, y_pred_test)\n\nprint(cf_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:13:02.02804Z","iopub.execute_input":"2022-02-22T20:13:02.028346Z","iopub.status.idle":"2022-02-22T20:13:02.045089Z","shell.execute_reply.started":"2022-02-22T20:13:02.028316Z","shell.execute_reply":"2022-02-22T20:13:02.044192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graphs for results","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g', annot_kws={\"fontsize\":22})\nax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ');\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(['0','1'])\nax.yaxis.set_ticklabels(['0','1'])\n## Display the visualization of the Confusion Matrix.\n\n\nplt.savefig('confusion_matrix.png', bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:13:03.266361Z","iopub.execute_input":"2022-02-22T20:13:03.26736Z","iopub.status.idle":"2022-02-22T20:13:03.735354Z","shell.execute_reply.started":"2022-02-22T20:13:03.267295Z","shell.execute_reply":"2022-02-22T20:13:03.734364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances = pd.Series(lr_model.coef_[0], index=X.columns)\ndf = pd.DataFrame(feat_importances).reset_index(drop=False)\ndf.columns = ['feat', 'imp']\ndf['abs_coef'] = df['imp'].apply(lambda x: abs(x))\ndf.sort_values(by='abs_coef', ascending=True, inplace = True)\ndf = df.reset_index(drop=True)\ndf['geo'] = pd.Series(df['feat'].values).str.contains('location|neighbourhood|dist|room_type|property_type')\ndf = df[df['geo'] == True]\n# df = df.drop(['host_response_time_within a day', 'host_response_time_within a few hour', 'host_response_time_within an hour'], axis=1)\ndf=df.head(15)\nfig = px.bar(x=df.imp, y=df.feat, orientation='h')\nfig.update_layout(\n    yaxis_title=\"Feature\",\n    xaxis_title = \"Effect Size (Normalized)\",\n    title = \"Top Effect Sizes for Predicting Location Rating\")\n\nplotly.offline.plot(fig, filename='feat_imp.html')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:13:28.861864Z","iopub.execute_input":"2022-02-22T20:13:28.862325Z","iopub.status.idle":"2022-02-22T20:13:28.978543Z","shell.execute_reply.started":"2022-02-22T20:13:28.862273Z","shell.execute_reply":"2022-02-22T20:13:28.977752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" from sklearn.metrics import PrecisionRecallDisplay\n\ndisp = PrecisionRecallDisplay(precision=lr_precision, recall=lr_recall)\ndisp.plot()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:13:59.749784Z","iopub.execute_input":"2022-02-22T20:13:59.750115Z","iopub.status.idle":"2022-02-22T20:13:59.976283Z","shell.execute_reply.started":"2022-02-22T20:13:59.750084Z","shell.execute_reply":"2022-02-22T20:13:59.975211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}